{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T12:23:01.417162Z",
     "start_time": "2021-03-29T12:23:01.406409Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import scipy\n",
    "import json\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from rich.progress import track\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Parameters\n",
    "min_word_freq = 5\n",
    "win_size = 3\n",
    "embed_dim = 100\n",
    "\n",
    "data_path = \"/home/data-master/evancw/flatten_gigaword/tmp/\" # data file in Orion\n",
    "# data_path = \"/home/jim/coding/Research/WordEmbed/dataset/\"     # data file in WSL\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from arnoldi import arnoldi_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T12:29:38.135898Z",
     "start_time": "2021-03-29T12:23:06.387431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a794769a2349fbb81284bed1a48720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = []\n",
    "with open(\"./tmp/line_corpus.txt\", 'r') as lc:\n",
    "    for line in track(lc.readlines()):\n",
    "        corpus.append(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T08:34:59.174173Z",
     "start_time": "2021-03-27T08:34:58.085609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# vocab:\t 450781\n"
     ]
    }
   ],
   "source": [
    "# Construct vocabulary & frequency statics\n",
    "\n",
    "def get_vocab(corpus: list):\n",
    "    vocab_freq = defaultdict(lambda: 0)\n",
    "    for sentence in track(corpus, description=\"Collecting words...\"):\n",
    "        for word in sentence:\n",
    "            vocab_freq[word] += 1\n",
    "    return vocab_freq\n",
    "\n",
    "\n",
    "def get_freq(w: str):\n",
    "    return vocab_freq[w]\n",
    "\n",
    "if not os.path.exists('./tmp/nyt_vocab.json'):\n",
    "    vocab_freq = get_vocab(corpus)\n",
    "    vocab = [word for word in vocab_freq.keys() if\n",
    "             get_freq(word) >= min_word_freq]\n",
    "    vocab = {word:i for i, word in enumerate(vocab)}\n",
    "    with open('./tmp/nyt_vocab.json', 'w') as f:\n",
    "        json.dump(vocab, f, indent=6)\n",
    "    with open('./tmp/nyt_vocab_freq.json', 'w') as f:\n",
    "        json.dump(vocab_freq, f, indent=6)\n",
    "else:\n",
    "    with open('./tmp/nyt_vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    with open('./tmp/nyt_vocab_freq.json', 'r') as f:\n",
    "        vocab_freq = json.load(f)\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(\"# vocab:\\t\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T08:35:00.877176Z",
     "start_time": "2021-03-27T08:34:59.176197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constrain the length of vocab to be 300,000.\n",
    "max_vocab = 300000\n",
    "sorted_vocab = dict(sorted(vocab_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "freq_vocab_set = set(list(sorted_vocab)[:max_vocab])\n",
    "freq_vocab = {x: vocab[x] for x in freq_vocab_set}\n",
    "freq_vocab = dict(sorted(freq_vocab.items(), key=lambda x:x[1]))\n",
    "\n",
    "# rearrange the new vocab (freq_vocab)\n",
    "vocab_ = {k: i for i, k in enumerate(freq_vocab.keys())}\n",
    "len(vocab_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:48:05.757323Z",
     "start_time": "2021-03-27T08:35:00.878856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31464564660b4251b22cdda9999f1fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct co-occurence matrix\n",
    "# Note: The comatrix contains all the vocab in the corpus. i.e. It does not neglect any Out of Vocabulary word. To constrain the vocabulary,\n",
    "#       use the subset of the matrix M\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_comatrix(corpus, win_size, word_dict):\n",
    "    vocab_len = len(word_dict)\n",
    "    coo = defaultdict(lambda: 0)\n",
    "    for sent in track(corpus, description=\"Extracting Co-Occurrence Matrix...\\t\"):\n",
    "        words = sent\n",
    "        sent_len = len(words)\n",
    "        for i in range(sent_len):\n",
    "            word = words[i]\n",
    "            try:\n",
    "                word_idx = word_dict[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            win_left = max(0, i - win_size)\n",
    "            win_right = min(sent_len, i + win_size)\n",
    "            contexts = words[win_left:i] + words[i + 1:win_right]\n",
    "            for context in contexts:\n",
    "                try:\n",
    "                    context_idx = word_dict[context]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                coo[(word_idx, context_idx)] += 1\n",
    "    coordinate = np.array(list(coo.keys())).T\n",
    "    data = np.array(list(coo.values()))\n",
    "    W = scipy.sparse.csr_matrix((data, coordinate), shape=(vocab_len, vocab_len), dtype=np.float32)\n",
    "    return W\n",
    "\n",
    "# construct&save/load Co-occurence Matrix\n",
    "import scipy\n",
    "if not os.path.exists(f\"./tmp/nyt_M_{win_size}.npz\"):\n",
    "    M = get_comatrix(corpus, win_size, vocab)\n",
    "    scipy.sparse.save_npz(f'./tmp/nyt_M_{win_size}.npz', M)\n",
    "else:\n",
    "    M = scipy.sparse.load_npz(f\"./tmp/nyt_M_{win_size}.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:48:07.172782Z",
     "start_time": "2021-03-27T09:48:05.760288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 300000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capture the comatrix w.r.t. the most frequent <max_vocab> words.\n",
    "indices = np.array(list(freq_vocab.values()))\n",
    "M_ = (M[indices].T)[indices].T\n",
    "M_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:48:07.788012Z",
     "start_time": "2021-03-27T09:48:07.174701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Square root of the matrix M before Arnoldi iteration\n",
    "sqrt_M = M_.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:49:49.346528Z",
     "start_time": "2021-03-27T09:48:07.792115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef93bb5b33d14fdd9fdaf304f9c2db49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct word embeddings by Arnoldi iteration\n",
    "if os.path.exists(f\"./tmp/nyt_Q_{win_size}.npy\"):\n",
    "    Q = np.load(f\"./tmp/nyt_Q_{win_size}.npy\")\n",
    "else:\n",
    "    b = np.random.random(size=max_vocab)  # initial vector\n",
    "    Q, h = arnoldi_iteration(sqrt_M, b, embed_dim)\n",
    "    np.save(f\"./tmp/nyt_Q_{win_size}.npy\", Q)  # save Word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-22T12:32:41.645Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T09:50:04.906899Z",
     "start_time": "2021-03-27T09:49:49.348952Z"
    }
   },
   "outputs": [],
   "source": [
    "dim = embed_dim\n",
    "Q_ = Q[:, :dim]\n",
    "WE_ = normalize(Q_, axis=1, norm=\"l2\")\n",
    "kv = KeyedVectors(vector_size=dim)\n",
    "kv.add(list(vocab_.keys()), WE_)\n",
    "kv.save_word2vec_format(f\"./tmp/arnodi_{dim}_{win_size}.kv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
