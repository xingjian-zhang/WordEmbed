{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.load(\"embed/brown_100_-1/Q.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8418, 101)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = json.load(open(\"embed/brown_100_-1/vocab.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {v:k for k,v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(Q, w:str, n:int=10):\n",
    "    try:\n",
    "        idx = vocab[w]\n",
    "    except KeyError:\n",
    "        print(\"Not found\", w)\n",
    "        return {}\n",
    "    inner = Q @ Q[idx]\n",
    "    n += 1\n",
    "    closest = np.argpartition(inner, -n)[-n:]\n",
    "    closest = closest[np.argsort(inner[closest])][:-1]\n",
    "    return {inv_vocab[cidx]:inner[cidx] for cidx in np.flip(closest)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titled': 0.79633653,\n",
       " 'cylindrical': 0.7635322,\n",
       " 'enters': 0.7552502,\n",
       " 'athlete': 0.75411177,\n",
       " 'eleanor': 0.7513427,\n",
       " 'tackle': 0.7495686,\n",
       " 'miscellaneous': 0.7469099,\n",
       " 'capitalism': 0.7435185,\n",
       " 'hints': 0.7434266,\n",
       " 'danced': 0.74307805}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(Q_, \"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "Q_ = normalize(Q, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(Q_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Notice how the variations, map to the same canonical form.\n",
    "\n",
    "In my experience, text normalization has even been effective for analyzing highly unstructured clinical texts where physicians take notes in non-standard ways. I’ve also found it useful for topic extraction where near synonyms and spelling differences are common (e.g. topic modelling, topic modeling, topic-modeling, topic-modelling).\n",
    "\n",
    "Unfortunately, unlike stemming and lemmatization, there isn’t a standard way to normalize texts. It typically depends on the task. For example, the way you would normalize clinical texts would arguably be different from how you normalize sms text messages.\n",
    "\n",
    "Some common approaches to text normalization include dictionary mappings (easiest), statistical machine translation (SMT) and spelling-correction based approaches. This interesting article compares the use of a dictionary based approach and a SMT approach for normalizing text messages.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from utils import preprocess_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notice how the variations map to the same canonical form\n",
      "in my experience text normalization has even been effective for analyzing highly unstructured clinical texts where physicians take notes in non standard ways\n",
      "i ve also found it useful for topic extraction where near synonyms and spelling differences are common e g\n",
      "topic modelling topic modeling topic modeling topic modelling\n",
      "unfortunately unlike stemming and lemmatization there isn t a standard way to normalize texts\n",
      "it typically depends on the task\n",
      "for example the way you would normalize clinical texts would arguably be different from how you normalize sms text messages\n",
      "some common approaches to text normalization include dictionary mappings easiest statistical machine translation smt and spelling correction based approaches\n",
      "this interesting article compares the use of a dictionary based approach and a smt approach for normalizing text messages\n"
     ]
    }
   ],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(preprocess_sent(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
